{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import pyodbc\n",
    "from unidecode import unidecode\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import year, month\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, udf, expr\n",
    "from pyspark.sql.types import StringType\n",
    "import urllib.parse\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extração dos dados \n",
    "\n",
    "Os dados foram extraídos pela ANTAQ para os anos de 2021, 2022 e 2023 e compilados utilizando o script abaixo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extração automatizada via BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando https://web3.antaq.gov.br/ea/txt/2021Atracacao.zip...\n",
      "Salvo: dados_brutos\\Atracacao_2021.zip\n",
      "Baixando https://web3.antaq.gov.br/ea/txt/2021Carga.zip...\n",
      "Salvo: dados_brutos\\Carga_2021.zip\n",
      "Baixando https://web3.antaq.gov.br/ea/txt/2021CargaConteinerizada.zip...\n",
      "Salvo: dados_brutos\\CargaConteinerizada_2021.zip\n",
      "Baixando https://web3.antaq.gov.br/ea/txt/2022Atracacao.zip...\n",
      "Salvo: dados_brutos\\Atracacao_2022.zip\n",
      "Baixando https://web3.antaq.gov.br/ea/txt/2022Carga.zip...\n",
      "Salvo: dados_brutos\\Carga_2022.zip\n",
      "Baixando https://web3.antaq.gov.br/ea/txt/2022CargaConteinerizada.zip...\n",
      "Salvo: dados_brutos\\CargaConteinerizada_2022.zip\n",
      "Baixando https://web3.antaq.gov.br/ea/txt/2023Atracacao.zip...\n",
      "Salvo: dados_brutos\\Atracacao_2023.zip\n",
      "Baixando https://web3.antaq.gov.br/ea/txt/2023Carga.zip...\n",
      "Salvo: dados_brutos\\Carga_2023.zip\n",
      "Baixando https://web3.antaq.gov.br/ea/txt/2023CargaConteinerizada.zip...\n",
      "Salvo: dados_brutos\\CargaConteinerizada_2023.zip\n"
     ]
    }
   ],
   "source": [
    "DOWNLOAD_URL = \"https://web3.antaq.gov.br/ea/txt/\"\n",
    "\n",
    "anos = [\"2021\", \"2022\", \"2023\"] #Modificar caso necessário outros períodos\n",
    "arquivos_desejados = [\"Atracacao\", \"Carga\", \"CargaConteinerizada\"]\n",
    "\n",
    "\n",
    "os.makedirs(\"dados_brutos\", exist_ok=True)\n",
    "\n",
    "def baixar_arquivos():\n",
    "    for ano in anos:\n",
    "        for arquivo_desejado in arquivos_desejados:\n",
    "            salvar_caminho = os.path.join(\"dados_brutos\", f\"{arquivo_desejado}_{ano}.zip\")\n",
    "            if os.path.exists(salvar_caminho):\n",
    "                print(f\"Arquivo {salvar_caminho} já existe. Pulando...\")\n",
    "                continue\n",
    "            \n",
    "            arquivo_url = f\"{DOWNLOAD_URL}{ano}{arquivo_desejado}.zip\"\n",
    "\n",
    "            print(f\"Baixando {arquivo_url}...\")\n",
    "            with open(salvar_caminho, \"wb\") as f:\n",
    "                f.write(requests.get(arquivo_url).content)\n",
    "            \n",
    "            print(f\"Salvo: {salvar_caminho}\")\n",
    "\n",
    "\n",
    "baixar_arquivos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descompactar os arquivos extraídos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descompactado: dados_brutos\\Atracacao_2021.zip -> dados_extraidos\\Atracacao_2021\n",
      "Descompactado: dados_brutos\\Atracacao_2022.zip -> dados_extraidos\\Atracacao_2022\n",
      "Descompactado: dados_brutos\\Atracacao_2023.zip -> dados_extraidos\\Atracacao_2023\n",
      "Descompactado: dados_brutos\\CargaConteinerizada_2021.zip -> dados_extraidos\\CargaConteinerizada_2021\n",
      "Descompactado: dados_brutos\\CargaConteinerizada_2022.zip -> dados_extraidos\\CargaConteinerizada_2022\n",
      "Descompactado: dados_brutos\\CargaConteinerizada_2023.zip -> dados_extraidos\\CargaConteinerizada_2023\n",
      "Descompactado: dados_brutos\\Carga_2021.zip -> dados_extraidos\\Carga_2021\n",
      "Descompactado: dados_brutos\\Carga_2022.zip -> dados_extraidos\\Carga_2022\n",
      "Descompactado: dados_brutos\\Carga_2023.zip -> dados_extraidos\\Carga_2023\n"
     ]
    }
   ],
   "source": [
    "def descompactar_arquivos():\n",
    "    for arquivo in os.listdir(\"dados_brutos\"):\n",
    "        if arquivo.endswith(\".zip\"):\n",
    "            caminho_zip = os.path.join(\"dados_brutos\", arquivo)\n",
    "            destino = os.path.join(\"dados_extraidos\", arquivo.replace(\".zip\", \"\"))\n",
    "\n",
    "            os.makedirs(destino, exist_ok=True)\n",
    "\n",
    "            try:\n",
    "                with zipfile.ZipFile(caminho_zip, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(destino)\n",
    "                    print(f\"Descompactado: {caminho_zip} -> {destino}\")\n",
    "            except zipfile.BadZipFile:\n",
    "                print(f\"Erro: {caminho_zip} não é um arquivo ZIP válido.\")\n",
    "                os.remove(caminho_zip)  # Remove arquivos inválidos\n",
    "\n",
    "descompactar_arquivos()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OBS. No advento de algum problema de conexão e download automático dos arquivos, é possível pular a etapa anterior e partir do script abaixo. Contanto que os dados estejam salvos no diretório \"./dados_extraidos\"\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "União dos arquivos via PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"ConsolidarDadosANTAQ\").getOrCreate()\n",
    "\n",
    "input_dir = \"dados_extraidos\"\n",
    "\n",
    "arquivos = [\"Atracacao\", \"Carga\", \"CargaConteinerizada\"]\n",
    "anos = [\"2021\", \"2022\", \"2023\"]\n",
    "\n",
    "atracacao = None\n",
    "carga = None\n",
    "carga_cont = None\n",
    "\n",
    "for arquivo in arquivos:\n",
    "    input_files = [f\"{input_dir}/{arquivo}_{ano}\" for ano in anos]\n",
    "    \n",
    "    existing_files = []\n",
    "    for folder in input_files:\n",
    "        if os.path.exists(folder):\n",
    "            files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(\".txt\")]\n",
    "            existing_files.extend(files)\n",
    "    \n",
    "    if not existing_files:\n",
    "        print(f\"{arquivo} não encontrado. Indo para o próximo...\")\n",
    "        continue\n",
    "    \n",
    "    df = spark.read.option(\"header\", True).option(\"delimiter\", \";\").csv(existing_files)\n",
    "\n",
    "    if arquivo == \"Atracacao\":\n",
    "        atracacao = df\n",
    "    elif arquivo == \"Carga\":\n",
    "        carga = df\n",
    "    elif arquivo == \"CargaConteinerizada\":\n",
    "        carga_cont = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamento de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabela 01 (atracacao_fato)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrar apenas pelas colunas de interesse (tabela_atracacao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_atracacao = [\n",
    "    'IDAtracacao', 'Tipo de Navegação da Atracação', 'CDTUP', 'Nacionalidade do Armador', \n",
    "    'IDBerco', 'FlagMCOperacaoAtracacao', 'Berço', 'Terminal', 'Porto Atracação', \n",
    "    'Município', 'Apelido Instalação Portuária', 'UF', 'Complexo Portuário', \n",
    "    'SGUF', 'Tipo da Autoridade Portuária', 'Região Geográfica', 'Nº da Capitania', \n",
    "    'Nº do IMO', 'Data Atracação', 'Data Chegada', 'Data Desatracação', 'Data Início Operação', \n",
    "    'Data Término Operação', 'Tipo de Operação'\n",
    "]\n",
    "\n",
    "atracacao_fato = atracacao.select(*colunas_atracacao)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remover valores nulos/ausentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "atracacao_fato = atracacao_fato.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converter colunas para o tipo data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = ['Data Atracação', 'Data Chegada', 'Data Desatracação', 'Data Início Operação', 'Data Término Operação']\n",
    "\n",
    "for coluna in datas:\n",
    "    atracacao_fato = atracacao_fato.withColumn(coluna, to_timestamp(atracacao_fato[coluna], 'dd/MM/yyyy HH:mm:ss'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerar as colunas 'Ano da data de início da operação' e 'Mês da data de início da operação'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "atracacao_fato = atracacao_fato.withColumn('Ano da data de início da operação', year(atracacao_fato['Data Atracação']))\n",
    "\n",
    "atracacao_fato = atracacao_fato.withColumn('Mês da data de início da operação', month(atracacao_fato['Data Atracação']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabela 02 (carga_fato)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtrar apenas pelas colunas de interesse (tabelas carga e atracacao_fato)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_carga = [\n",
    "    'IDCarga', 'FlagTransporteViaInterioir', 'IDAtracacao', 'Percurso Transporte em vias Interiores', \n",
    "    'Origem', 'Percurso Transporte Interiores', 'Destino', 'STNaturezaCarga', \n",
    "    'CDMercadoria', 'STSH2', 'Tipo Operação da Carga', 'STSH4', 'Carga Geral Acondicionamento', \n",
    "    'Natureza da Carga', 'ConteinerEstado', 'Sentido', 'Tipo Navegação', 'TEU', 'FlagAutorizacao', \n",
    "    'QTCarga'\n",
    "]\n",
    "\n",
    "colunas_atracacao_fato = [\n",
    "    'IDAtracacao', 'Porto Atracação', 'SGUF', 'Ano da data de início da operação', 'Mês da data de início da operação'\n",
    "]\n",
    "\n",
    "carga_intermed = carga.select(*colunas_carga)\n",
    "atracacao_fato_intermed = atracacao_fato.select(*colunas_atracacao_fato)\n",
    "\n",
    "carga_fato = carga_intermed.join(atracacao_fato_intermed, on=\"IDAtracacao\", how=\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcular Peso líquido da carga:   \n",
    "carga_merged['VLPesoLiquido'] = carga_merged['VLPesoCargaBruta'] - carga_merged['VLPesoCargaConteinerizada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "carga = carga.dropna()\n",
    "carga_cont = carga_cont.dropna()\n",
    "\n",
    "carga = carga.withColumn(\"VLPesoCargaBruta\", F.regexp_replace(\"VLPesoCargaBruta\", \",\", \".\").cast(\"float\"))\n",
    "carga_cont = carga_cont.withColumn(\"VLPesoCargaConteinerizada\", F.regexp_replace(\"VLPesoCargaConteinerizada\", \",\", \".\").cast(\"float\"))\n",
    "\n",
    "carga_merged = carga.join(carga_cont, on=\"IDCarga\", how=\"left\")\n",
    "carga_merged = carga_merged.withColumn(\"VLPesoLiquido\", F.col(\"VLPesoCargaBruta\") - F.col(\"VLPesoCargaConteinerizada\"))\n",
    "\n",
    "carga_fato = carga_fato.join(carga_merged.select(\"IDCarga\", \"VLPesoLiquido\"), on=\"IDCarga\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remoção de valores nulos/ausentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "carga_fato = carga_fato.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remover acentuação, cedilha, espaços e outros caracteres nos nomes das variáveis para correto carregamento no banco de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coluna in atracacao_fato.columns:\n",
    "    novo_nome = unidecode(coluna)\n",
    "    atracacao_fato = atracacao_fato.withColumnRenamed(coluna, novo_nome)\n",
    "\n",
    "for coluna in carga_fato.columns:\n",
    "    novo_nome = unidecode(coluna)\n",
    "    carga_fato = carga_fato.withColumnRenamed(coluna, novo_nome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "atracacao_fato = atracacao_fato.select(\n",
    "    col(\"IDAtracacao\"),\n",
    "    col(\"Tipo de Navegacao da Atracacao\").alias(\"Tipo_Navegacao_Atracacao\"),\n",
    "    col(\"CDTUP\"),\n",
    "    col(\"Nacionalidade do Armador\").alias(\"Nacionalidade_Armador\"),\n",
    "    col(\"IDBerco\"),\n",
    "    col(\"FlagMCOperacaoAtracacao\"),\n",
    "    col(\"Berco\"),\n",
    "    col(\"Terminal\"),\n",
    "    col(\"Porto Atracacao\").alias(\"Porto_Atracacao\"),\n",
    "    col(\"Municipio\"),\n",
    "    col(\"Apelido Instalacao Portuaria\").alias(\"Apelido_Instalacao_Portuaria\"),\n",
    "    col(\"UF\"),\n",
    "    col(\"Complexo Portuario\").alias(\"Complexo_Portuario\"),\n",
    "    col(\"SGUF\"),\n",
    "    col(\"Tipo da Autoridade Portuaria\").alias(\"Tipo_Autoridade_Portuaria\"),\n",
    "    col(\"Regiao Geografica\").alias(\"Regiao_Geografica\"),\n",
    "    col(\"No da Capitania\").alias(\"No_Capitania\"),\n",
    "    col(\"No do IMO\").alias(\"No_IMO\"),\n",
    "    col(\"Data Atracacao\").alias(\"Data_Atracacao\"),\n",
    "    col(\"Data Chegada\").alias(\"Data_Chegada\"),\n",
    "    col(\"Data Desatracacao\").alias(\"Data_Desatracacao\"),\n",
    "    col(\"Data Inicio Operacao\").alias(\"Data_Inicio_Operacao\"),\n",
    "    col(\"Data Termino Operacao\").alias(\"Data_Termino_Operacao\"),\n",
    "    col(\"Tipo de Operacao\").alias(\"Tipo_Operacao\"),\n",
    "    col(\"Ano da data de inicio da operacao\").alias(\"Ano_Inicio_Operacao\"),\n",
    "    col(\"Mes da data de inicio da operacao\").alias(\"Mes_Inicio_Operacao\")\n",
    ")\n",
    "\n",
    "carga_fato = carga_fato.select(\n",
    "    col(\"IDCarga\"),\n",
    "    col(\"IDAtracacao\"),\n",
    "    col(\"Origem\"),\n",
    "    col(\"Destino\"),\n",
    "    col(\"CDMercadoria\"),\n",
    "    col(\"Tipo Operacao da Carga\").alias(\"Tipo_Operacao_Carga\"),\n",
    "    col(\"Carga Geral Acondicionamento\").alias(\"Carga_Geral_Acondicionamento\"),\n",
    "    col(\"ConteinerEstado\"),\n",
    "    col(\"Tipo Navegacao\").alias(\"Tipo_Navegacao\"),\n",
    "    col(\"FlagAutorizacao\"),\n",
    "    col(\"Percurso Transporte em vias Interiores\").alias(\"Percurso_Transporte_Vias_Interiores\"),\n",
    "    col(\"Percurso Transporte Interiores\").alias(\"Percurso_Transporte_Interiores\"),\n",
    "    col(\"STNaturezaCarga\"),\n",
    "    col(\"STSH2\"),\n",
    "    col(\"STSH4\"),\n",
    "    col(\"Natureza da Carga\").alias(\"Natureza_Carga\"),\n",
    "    col(\"Sentido\"),\n",
    "    col(\"TEU\"),\n",
    "    col(\"QTCarga\"),\n",
    "    col(\"VLPesoLiquido\").alias(\"VLPesoCargaBruta\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remover caracteres indesejados dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unidecode_udf = udf(lambda x: unidecode(x) if x is not None else x, StringType())\n",
    "\n",
    "def remover_acentos(df):\n",
    "    return df.select([expr(f\"translate({coluna}, 'áéíóúãõôç', 'aeiouaooc')\").alias(coluna) for coluna in df.columns])\n",
    "\n",
    "atracacao_fato = remover_acentos(atracacao_fato)\n",
    "carga_fato = remover_acentos(carga_fato)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualização das tabelas finalizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------------+-------+---------------------+-----------+-----------------------+-----------+--------------------+--------------------+---------+----------------------------+--------+--------------------+----+-------------------------+-----------------+------------+-------+-------------------+-------------------+-------------------+--------------------+---------------------+--------------------+-------------------+-------------------+\n",
      "|IDAtracacao|Tipo_Navegacao_Atracacao|  CDTUP|Nacionalidade_Armador|    IDBerco|FlagMCOperacaoAtracacao|      Berco|            Terminal|     Porto_Atracacao|Municipio|Apelido_Instalacao_Portuaria|      UF|  Complexo_Portuario|SGUF|Tipo_Autoridade_Portuaria|Regiao_Geografica|No_Capitania| No_IMO|     Data_Atracacao|       Data_Chegada|  Data_Desatracacao|Data_Inicio_Operacao|Data_Termino_Operacao|       Tipo_Operacao|Ano_Inicio_Operacao|Mes_Inicio_Operacao|\n",
      "+-----------+------------------------+-------+---------------------+-----------+-----------------------+-----------+--------------------+--------------------+---------+----------------------------+--------+--------------------+----+-------------------------+-----------------+------------+-------+-------------------+-------------------+-------------------+--------------------+---------------------+--------------------+-------------------+-------------------+\n",
      "|    1194922|               Cabotagem|BRAM009|                    1|BRAM0090002|                      1|Pier de GLP|Terminal Aquaviar...|Terminal Aquaviar...|    Coari|           Terminal de Coari|Amazonas|Coari (Instalacao...|  AM|      Terminal Autorizado|            Norte|  3813911888|9596870|2021-10-28 14:40:00|2021-10-28 13:45:00|2021-10-29 06:15:00| 2021-10-28 15:45:00|  2021-10-29 00:25:00|Movimentacao da C...|               2021|                 10|\n",
      "|    1213788|                Interior|BRAM011|                    1|BRAM0110003|                      1|      POF 3|Terminal Aquaviar...|Terminal Aquaviar...|   Manaus|          Terminal de Manaus|Amazonas|              Manaus|  AM|      Terminal Autorizado|            Norte|  3813911888|9596870|2021-11-04 16:55:00|2021-10-29 21:30:00|2021-11-07 02:20:00| 2021-11-04 20:00:00|  2021-11-06 23:35:00|Movimentacao da C...|               2021|                 11|\n",
      "|    1197543|               Cabotagem|BRAM009|                    1|BRAM0090002|                      1|Pier de GLP|Terminal Aquaviar...|Terminal Aquaviar...|    Coari|           Terminal de Coari|Amazonas|Coari (Instalacao...|  AM|      Terminal Autorizado|            Norte|  3813911888|9596870|2021-11-13 15:15:00|2021-11-13 14:00:00|2021-11-14 06:10:00| 2021-11-13 16:00:00|  2021-11-14 00:40:00|Movimentacao da C...|               2021|                 11|\n",
      "+-----------+------------------------+-------+---------------------+-----------+-----------------------+-----------+--------------------+--------------------+---------+----------------------------+--------+--------------------+----+-------------------------+-----------------+------------+-------+-------------------+-------------------+-------------------+--------------------+---------------------+--------------------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+--------+-----------+-------+-------+------------+-------------------+----------------------------+---------------+--------------+---------------+-----------------------------------+------------------------------+---------------+---------+---------+--------------------+-------------+---+-------+----------------+\n",
      "| IDCarga|IDAtracacao| Origem|Destino|CDMercadoria|Tipo_Operacao_Carga|Carga_Geral_Acondicionamento|ConteinerEstado|Tipo_Navegacao|FlagAutorizacao|Percurso_Transporte_Vias_Interiores|Percurso_Transporte_Interiores|STNaturezaCarga|    STSH2|    STSH4|      Natureza_Carga|      Sentido|TEU|QTCarga|VLPesoCargaBruta|\n",
      "+--------+-----------+-------+-------+------------+-------------------+----------------------------+---------------+--------------+---------------+-----------------------------------+------------------------------+---------------+---------+---------+--------------------+-------------+---+-------+----------------+\n",
      "|28269469|    1143110|BRRO005|BRAM007|        22G0|           Interior|              Conteinerizada|          Cheio|      Interior|              S|               Interior Interest...|            Navegacao Interior|      Exclusivo|Exclusivo|Exclusivo|Carga Conteinerizada|Desembarcados|  1|      1|             0.0|\n",
      "|28269482|    1143110|BRRO005|BRAM007|        42G0|           Interior|              Conteinerizada|          Cheio|      Interior|              S|               Interior Interest...|            Navegacao Interior|      Exclusivo|Exclusivo|Exclusivo|Carga Conteinerizada|Desembarcados|  2|      1|    0.0030002594|\n",
      "|28269489|    1143110|BRRO005|BRAM007|        42G0|           Interior|              Conteinerizada|          Cheio|      Interior|              S|               Interior Interest...|            Navegacao Interior|      Exclusivo|Exclusivo|Exclusivo|Carga Conteinerizada|Desembarcados|  2|      1|     -0.00399971|\n",
      "+--------+-----------+-------+-------+------------+-------------------+----------------------------+---------------+--------------+---------------+-----------------------------------+------------------------------+---------------+---------+---------+--------------------+-------------+---+-------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "atracacao_fato.show(3)\n",
    "carga_fato.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso necessário, salvar as tabelas em csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"dados_finais\", exist_ok=True)\n",
    "\n",
    "atracacao_fato.toPandas().to_csv(\"dados_finais/atracacao_fato.csv\", sep=\";\", index=False)\n",
    "carga_fato.toPandas().to_csv(\"dados_finais/carga_fato.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disponibilização em banco de dados (SQL Server)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuração da conexão (utilizando dotenv)\n",
    "\n",
    "OBS.: O arquivo.env está no mesmo diretório e precisará ser editado com as credenciais corretas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"arquivo.env\")\n",
    "\n",
    "server = os.getenv(\"SERVER\")\n",
    "database = os.getenv(\"DATABASE\")\n",
    "username = os.getenv(\"USERNAME\")\n",
    "password = os.getenv(\"PASSWORD\")\n",
    "\n",
    "password_encoded = urllib.parse.quote_plus(password)\n",
    "\n",
    "connection_string = f\"mssql+pyodbc://{username}:{password_encoded}@{server}/{database}?driver=SQL+Server\"\n",
    "\n",
    "engine = create_engine(connection_string, fast_executemany=True)\n",
    "\n",
    "atracacao_fato_pd = atracacao_fato.toPandas()\n",
    "carga_fato_pd = carga_fato.toPandas()\n",
    "\n",
    "def importar_dados_para_sql(df, tabela):\n",
    "    try:\n",
    "        df.to_sql(tabela, engine, if_exists=\"append\", index=False, chunksize=1000)\n",
    "        print(\"Dados importados com sucesso para a tabela {tabela}.\")\n",
    "    except Exception as e:\n",
    "        print(\"Erro ao importar os dados para a tabela {tabela}: {e}\")\n",
    "\n",
    "importar_dados_para_sql(atracacao_fato_pd, \"atracacao_fato\")\n",
    "importar_dados_para_sql(carga_fato_pd, \"carga_fato\")\n",
    "\n",
    "engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
