{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.operators.email import EmailOperator\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, year, month\n",
    "from sqlalchemy import create_engine\n",
    "import urllib.parse\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baixar arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baixar_arquivos():\n",
    "    DOWNLOAD_URL = \"https://web3.antaq.gov.br/ea/txt/\"\n",
    "    anos = [\"2021\", \"2022\", \"2023\"]\n",
    "    arquivos_desejados = [\"Atracacao\", \"Carga\", \"CargaConteinerizada\"]\n",
    "    os.makedirs(\"dados_brutos\", exist_ok=True)\n",
    "    for ano in anos:\n",
    "        for arquivo in arquivos_desejados:\n",
    "            caminho = f\"dados_brutos/{arquivo}_{ano}.zip\"\n",
    "            if not os.path.exists(caminho):\n",
    "                url = f\"{DOWNLOAD_URL}{ano}{arquivo}.zip\"\n",
    "                with open(caminho, \"wb\") as f:\n",
    "                    f.write(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descompactar arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descompactar_arquivos():\n",
    "    os.makedirs(\"dados_extraidos\", exist_ok=True)\n",
    "    for arquivo in os.listdir(\"dados_brutos\"):\n",
    "        if arquivo.endswith(\".zip\"):\n",
    "            caminho_zip = os.path.join(\"dados_brutos\", arquivo)\n",
    "            destino = os.path.join(\"dados_extraidos\", arquivo.replace(\".zip\", \"\"))\n",
    "            os.makedirs(destino, exist_ok=True)\n",
    "            with zipfile.ZipFile(caminho_zip, 'r') as zip_ref:\n",
    "                zip_ref.extractall(destino)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processar_dados():\n",
    "    spark = SparkSession.builder.appName(\"ConsolidarDadosANTAQ\").getOrCreate()\n",
    "\n",
    "    input_dir = \"dados_extraidos\"\n",
    "\n",
    "    arquivos = [\"Atracacao\", \"Carga\", \"CargaConteinerizada\"]\n",
    "    anos = [\"2021\", \"2022\", \"2023\"]\n",
    "\n",
    "    atracacao = None\n",
    "    carga = None\n",
    "    carga_cont = None\n",
    "\n",
    "    for arquivo in arquivos:\n",
    "        input_files = [f\"{input_dir}/{arquivo}_{ano}\" for ano in anos]\n",
    "        \n",
    "        existing_files = []\n",
    "        for folder in input_files:\n",
    "            if os.path.exists(folder):\n",
    "                files = [os.path.join(folder, f) for f in os.listdir(folder) if f.endswith(\".txt\")]\n",
    "                existing_files.extend(files)\n",
    "        \n",
    "        if not existing_files:\n",
    "            print(f\"{arquivo} não encontrado. Indo para o próximo...\")\n",
    "            continue\n",
    "        \n",
    "        df = spark.read.option(\"header\", True).option(\"delimiter\", \";\").csv(existing_files)\n",
    "\n",
    "        if arquivo == \"Atracacao\":\n",
    "            atracacao = df\n",
    "        elif arquivo == \"Carga\":\n",
    "            carga = df\n",
    "        elif arquivo == \"CargaConteinerizada\":\n",
    "            carga_cont = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_dados():\n",
    "    load_dotenv(\"arquivo.env\")\n",
    "\n",
    "    server = os.getenv(\"SERVER\")\n",
    "    database = os.getenv(\"DATABASE\")\n",
    "    username = os.getenv(\"USERNAME\")\n",
    "    password = os.getenv(\"PASSWORD\")\n",
    "\n",
    "    password_encoded = urllib.parse.quote_plus(password)\n",
    "\n",
    "    connection_string = f\"mssql+pyodbc://{username}:{password_encoded}@{server}/{database}?driver=SQL+Server\"\n",
    "\n",
    "    engine = create_engine(connection_string, fast_executemany=True)\n",
    "\n",
    "    atracacao_fato_pd = atracacao_fato.toPandas()\n",
    "    carga_fato_pd = carga_fato.toPandas()\n",
    "\n",
    "    def importar_dados_para_sql(df, tabela):\n",
    "        try:\n",
    "            df.to_sql(tabela, engine, if_exists=\"append\", index=False, chunksize=1000)\n",
    "            print(\"Dados importados com sucesso para a tabela {tabela}.\")\n",
    "        except Exception as e:\n",
    "            print(\"Erro ao importar os dados para a tabela {tabela}: {e}\")\n",
    "\n",
    "    importar_dados_para_sql(atracacao_fato_pd, \"atracacao_fato\")\n",
    "    importar_dados_para_sql(carga_fato_pd, \"carga_fato\")\n",
    "\n",
    "    engine.dispose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_dados():\n",
    "    return os.path.exists(\"dados_extraidos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notificar_sucesso():\n",
    "    print(\"ETL finalizado\")\n",
    "\n",
    "def notificar_falha():\n",
    "    print(\"ETL falhou\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'antaq_etl',\n",
    "    default_args=default_args,\n",
    "    description='DAG para ETL da base ANTAQ',\n",
    "    schedule_interval='@daily',\n",
    ")\n",
    "\n",
    "task_baixar = PythonOperator(\n",
    "    task_id='baixar_arquivos',\n",
    "    python_callable=baixar_arquivos,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "task_descompactar = PythonOperator(\n",
    "    task_id='descompactar_arquivos',\n",
    "    python_callable=descompactar_arquivos,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "task_processar = PythonOperator(\n",
    "    task_id='processar_dados',\n",
    "    python_callable=processar_dados,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "task_carregar = PythonOperator(\n",
    "    task_id='carregar_dados',\n",
    "    python_callable=carregar_dados,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "task_verificar = PythonOperator(\n",
    "    task_id='verificar_dados',\n",
    "    python_callable=verificar_dados,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "task_sucesso = EmailOperator(\n",
    "    task_id='notificar_sucesso',\n",
    "    to='email@example.com',\n",
    "    subject='ETL Finalizado com Sucesso',\n",
    "    html_content='O processo ETL foi concluído com sucesso.',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "task_falha = EmailOperator(\n",
    "    task_id='notificar_falha',\n",
    "    to='email@example.com',\n",
    "    subject='ETL Falhou',\n",
    "    html_content='O processo ETL falhou.',\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "task_baixar >> task_descompactar >> task_processar >> task_carregar >> task_verificar\n",
    "\n",
    "task_verificar >> task_sucesso\n",
    "\n",
    "task_verificar >> task_falha\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
